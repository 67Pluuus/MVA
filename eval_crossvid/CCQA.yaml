# 路径配置
paths:
  Benchmark_name: "Benchmark/CrossVid"
  video_base_dir: "Benchmark/CrossVid/videos"
  json_file: "Benchmark/CrossVid/QA/CCQA_1.json"

# 模型配置
models:
  main_model_path: "Qwen/Qwen3-VL-2B-Instruct"
  api_key: "7117a451621443b1b550a08acadad5e1.Mljp89b7Rwz7SeiA"

# 运行参数
parameters:
  describe_frames: 8
  num_gpus: 1
  # 任务数量, -1 表示所有
  num_tasks: -1
  save_key_frames: false
  save_video_frames: false
  number_type: "ABC"

  # Agent 参数
  agent:
    global_max_iterations: 20 # 通过Tool Agent获取信息的全局最大迭代次数
    skip_iteration: false # 是否跳过初始化后的迭代过程 (仅用于测试或者快速预览)
    frame_bank_size: 16 # 每个视频最多取这么多帧
    initial_acceleration: 1.0

  type_watermark: "tag" # 水印类型
  # 可选模式 (可叠加, 如"tag, trans"):
  # "tag": 视频标签 + 时间戳 (左上角)
  # "trans": 插入转场帧 (黑屏+文字预告下一帧)
  # "temporal": 视频标签 + 底部进度条 (显示当前时间位置)
  # "none": 无水印
  priority_score_k: 1.0 # 优先级计算中加速权重k的值，默认1.0
  priority_score_t: 1.0 # 优先级计算中当前分数权重t的值，默认1.0

# 提示词
prompts:
  # 最终回答提示词
  answer: |
    /no_think
    /*Task Description*/
    You are an answer generation model in a multi-video Q&A task. You will receive multiple keyframes from different videos. Each frame has a label in the upper left corner indicating which video it belongs to (e.g., "Video 1", "Video 2", etc.), and the corresponding timestamp in this video.

    You are also provided with textual descriptions for each video:
    {DESCRIPTIONS}

    Question: "{QUESTION}"
    Options: "{OPTIONS}"

    Your task is to analyze all the provided keyframes AND the video descriptions to answer the question. Pay attention to:
    1. The visual content of each frame
    2. The textual summary of each video
    3. Which video each frame belongs to (indicated by the label)
    4. The differences and similarities between frames from different videos
    5. The specific details relevant to answering the question

    **Output Format:**
    Output only your answer in less than 100 words.

  tool_decide_action: |
    /no_think
    /* Task Description */
    You are a Tool Agent in a multi-video Q&A task. You will receive a set of sampled frames from **{VIDEO_LABEL}**. Your goal is to navigate through this video to find visual evidence that answers a specific question.
    Question: "{QUESTION}"

    You are currently observing a set of sampled frames from this video.
    Current Receptive Field: {START_TIME:.2f}s to {END_TIME:.2f}s (The time range covered by the current frame bank).
    Video Duration: {VIDEO_DURATION:.2f}s.
    Global Receptive Field Covered: {IS_GLOBAL}

    Based on the current visual information and the question, you need to decide the next exploration strategy to better locate the answer.

    /* Options */
    1. **Focus Middle, Replace current frames**: 
       - Select a narrower time range within the current field to zoom in. 
       - **DISCARD** all current frames and only keep the newly sampled frames from the target range.
    2. **Focus Middle, Keep current frames**: 
       - Select a narrower time range within the current field to zoom in.
       - **RETAIN** current frames and ADD newly sampled frames to the bank.
    3. **Focus Start/End, Replace current frames** (Only if Global): 
       - Focus on the beginning or end of the video if the middle part is irrelevant.
       - **DISCARD** all current frames and only keep the newly sampled frames.
    4. **Focus Start/End, Keep current frames** (Only if Global): 
       - Focus on the beginning or end of the video.
       - **RETAIN** current frames and ADD newly sampled frames.
    5. **Global Uniform Sampling with offset, Replace current frames**: 
       - Re-sample uniformly across the entire video duration with a random offset to capture diverse content.
       - **DISCARD** all current frames and only keep the new uniform frames.
    6. **Terminate video exploration**: 
       - Stop exploring this video if you believe no further useful information can be found or you have found sufficient evidence.

    /* Output Format */
    Choose an option (1-6). If choosing 1-4, also specify the target time range [start, end] within the current receptive field.
    Output exactly in this format:
    Option: <number>
    Range: [<start>, <end>]

  tool_score_frames: |
    /no_think
    /* Task Description */
    You are a Tool Agent in a multi-video Q&A task. You are analyzing **{VIDEO_LABEL}** to answer a question.
    Question: "{QUESTION}"

    You are provided with {NUM_FRAMES} frames from this video. Each frame represents a specific timestamp.
    Your task is to evaluate the **importance** of each frame in answering the question.

    /* Scoring Criteria */
    - **High Score (>0.7)**: The frame contains critical visual evidence that directly helps answer the question.
    - **Low Score (<0.3)**: The frame is irrelevant, redundant, or blurry, and should be discarded to improve efficiency.
    - **Avoid Indecision**: Do NOT give ambiguous scores like 0.5. Be decisive.
    - High-scoring frames are more likely to be retained in future steps, while low-scoring frames are more likely to be discarded.

    /* Output Format */
    Output exactly {NUM_FRAMES} floating-point numbers between 0.00 and 1.00, separated by spaces.
    Example output:
    0.10 0.85 0.40 0.90 0.20 0.15 0.70 0.50

  tool_generate_raw_description: |
    /no_think
    /* Task Description */
    You are a Tool Agent in a multi-video Q&A task. You are analyzing frames from **{VIDEO_LABEL}**.
    Question: "{QUESTION}"

    You are provided with a set of frames from this video.
    Your task is to generate a **preliminary description** of the visual content in these frames.

    /* Requirements */
    - **Focus on the Question**: Describe ONLY the visual details that are relevant to answering the question.
    - **Be Factural**: Do not hallucinate. Describe what is visible.
    - **Be Concise**: Provide a dense summary of important visual cues.

    Output the description directly, without any additional text.

  desc_refine_and_evaluate: |
    /no_think
    /* Task Description */
    You are a Description Agent in a multi-video Q&A task. Your goal is to maintain an accurate and concise description of **{VIDEO_LABEL}** to help answer a question.
    Question: "{QUESTION}"

    /* Context */
    1. **Previous Description**: "{DESC_OLD}" (Your prior understanding of this video).
    2. **New Observation**: "{DESC_RAW}" (A preliminary description from new frames).
    3. **Other Videos**: 
    {OTHER_DESCS_TEXT} (What we know about other videos in the task).

    /* Tasks */
    1. **Information Fusion**: 
       - Merge the "Previous Description" and "New Observation".
       - Keep critical details that answer the question.
       - Remove outdated or irrelevant information.
       - Ensure the new description is coherent.

    2. **Quality Evaluation (Score 0.00-1.00)**:
       - Rate how helpful the refined description is for answering the question.
       - High score = The description contains the answer or strong evidence.
       - Low score = The description is vague or irrelevant.

    3. **Termination Check 1 (Current Video)**:
       - have we fully mined ALL useful information from THIS video and no longer need to explore it? (True/False)

    4. **Termination Check 2 (Global Task)**:
       - Combining information from THIS video and OTHER videos, do we have enough evidence to answer the question conclusively? (True/False)

    /* Output Format */
    Refined Description: <your refined description>
    Score: <score>
    Video Terminated: <True/False>
    Global Terminated: <True/False>
